%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{flairs}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{setspace}
\frenchspacing
\usepackage{verbatim}

\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

\newcommand{\mytilde}{\raisebox{0.4ex}{\texttildelow}}
%----Making things more compact
\newcommand{\smalltt}[1]{{\small \tt #1}}
\newenvironment{packed_itemize}{
\vspace*{-0.2em}
\begin{itemize}
\setlength{\partopsep}{0pt}
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
}{\end{itemize}}
\newenvironment{packed_enumerate}{
\vspace*{-0.2em}
\begin{enumerate}
\setlength{\partopsep}{0pt}
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
}{\end{enumerate}}
\renewcommand{\textfraction}{0.07}
\renewcommand{\topfraction}{0.9}
\renewcommand{\bottomfraction}{0.9}
\renewcommand{\floatpagefraction}{0.66}
\setlength{\floatsep}{2.0pt plus 2.0pt minus 2.0pt}
\setlength{\textfloatsep}{10.0pt plus 2.0pt minus 0.0pt}

% \pdfinfo{
% /Title (ShZZaM: An LLM+ATP Natural Language to Logic Translator)
% /Author (Geoff Sutcliffe, Danial Haroon)}
% \setcounter{secnumdepth}{2}  
% \begin{document}
% \title{ShZZaM: An LLM+ATP Natural Language to Logic Translator}
% \author{Geoff Sutcliffe, Danial Haroon\\
% University of Miami\\
% Coral Gables, USA
% }
\pdfinfo{
/Title (ShZZaM: An LLM+ATP Natural Language to Logic Translator)
/Author (First1 Last1, First2 Last2)}
\setcounter{secnumdepth}{2}  
\begin{document}
\title{ShZZaM: An LLM+ATP Natural Language to Logic Translator}
\author{First1 Last1, First2 Last2\\
Affiliation\\
City, Country
}
\maketitle
\begin{abstract}
\begin{quote}
This paper describes the ShZZaM tool that uses Large Language Models (LLMs) and Automated
Theorem Proving (ATP) tools to translate natural language to typed first-order logic in
the TFF syntax of the TPTP World.
\end{quote}
\end{abstract}
%--------------------------------------------------------------------------------------------------
Large Language Models (LLMs)~\cite{PR+25} have shown themselves to be useful in a broad range
of applications~\cite{MK+25}.
However, it is well known that LLMs make mistakes~\cite{HY+25}, and this is acknowledged on 
LLMs' web interfaces, e.g., ChatGPT admits "ChatGPT can make mistakes. Check important info".
In the face of such unreliability, the results from LLMs in mission-critical applications require
verification.
One approach is to translate the LLM input and output to a logical form that can be checked
using Automated Theorem Proving (ATP) tools, e.g.,~\cite{YS+25,CL+25}.\footnote{%
For a more comprehensive survey, just ask your favourite LLM to ``show me some research on how LLMs 
make mistakes, and the need for symbolic checking of LLM output''.}
A key step in this verification pipeline is the faithful translation of the natural language to
an appropriate logical form.
This task is difficult due to the ambiguous nature of natural language statements, especially
informally expressed statements.
Work in this area includes LINC~\cite{OG+23}, FOLIO~\cite{HS+24}, and LINA~\cite{LL+24-arXiv}.
This paper makes another contribution in this area, taking a new interactive approach to the
translation process, zigzagging (hence the 'ZZ' in the tool name) between natural language and 
logic until convergence is achieved.
A key feature of ShZZaM is its use of LLMs and Automated Theorem Proving (ATP) tools, which
complement each other in the translation steps.

Figure~\ref{Process} shows the overall process implemented of ShZZaM.
Starting with the natural language, a combination of LLMs and ATP tools make a first translation
(LLM-L+ATP - a ``Zig'') to the typed first-order logic in the TFF syntax~\cite{SS+12,BP13-TFF1} of 
the TPTP World~\cite{Sut24}.
An LLM is then used to translate the logic back to natural language (LLM-NL - a ``Zag'').
An LLM is then used to judge (LLM-S) the similarity in meaning of the new and previous natural
language statements.
If they are adequately similar - above a ``convergence threshold'', the logic inbetween them 
is accepted as the translation.
This zizagging continues until the natural language pairs converge to the required level of
similarity (or a limit is reached).
Upon convergence the logic is sent to an ATP system via the SystemOnTPTP 
service~\cite{Sut00-CADE-17}, either a model finder if there are only axioms in the logic, or a
theorem prover if there is also a conjecture.
The results from the ATP system is reported in the SZS format~\cite{Sut08-KEAPPA}.
If the similarity between the final natural language and the original natural language
(which is computed in the zigzag step - see below) is above the ``zigzagging threshold'' the
translation is complete.
Otherwise the entire process repeats (or a limit is reached).
This outermost loop ensures the final natural language of the converged pair is adequately
similar in meaning to the original natural language.

\begin{figure*}[bt]
\centering
\includegraphics[width=\textwidth]{Process.pdf}
\caption{ShZZaM process}
\label{Process}
\end{figure*}

The translation from natural language to TFF logic and back to natural language - one zigzag, is 
an iterative one involving LLMs and ATP tools.
Figure~\ref{ZigZag} shows the details.
LLM-L is used to translate from natural language to logic.
The translation is successively checked using ATP tools for syntax errors and type errors (recall 
the logic is {\em typed} first-order logic).
If an error occurs in either check the error message is captured and passed back into the LLM-L 
for another attempt.
When a syntactically and type correct logic is created, LLM-NL translates the logic back to
the provisional natural language, and LLM-S is used to compare this to the original natural 
language.
If the similarity is below an ``acceptance threshold'' the provisional natural language is rejected
and the error is passed back into the LLM-L for another attempt.
This prevents the new natural language straying too far in meaning from the original natural
language.
If the similarity is below the acceptance threshold the provisional natural language becomes the
accepted result of the zigzag.
It is this accepted natural language that is compared to the previous natural language for
convergence, as explained above.

\begin{figure*}[bt]
\centering
\includegraphics[width=\textwidth]{ZigZag.pdf}
\caption{Details of one zigzag}
\label{ZigZag}
\end{figure*}

The LLM translation from natural language to logic and back again works because the LLM has
been exposed to enough natural language and enough TPTP format TFF logic.
The former is the natural result of scraping the world's web sites, etc.
The latter might be surprising, as TFF is a comparatively small fragment of the data used to train
LLM models.
Evidently there are adequate corpora that use TFF that are exposed on the web, e.g., the
TPTP problem library~\cite{Sut17}, exports from the Isabelle Archive of Formal 
Proofs~\cite{BH+15}, exports of the Mizar Mathematical Library~\cite{Urb03}, etc.

ShZZaM is implemented in Python.\footnote{%
Available, with test files and the results discussed below, from 
\smalltt{https://github.com/GeoffsPapers/ShZZaM}.}
ShZZaM has parameters that allow the selection of LLMs (default OpenAI's {\tt gpt-5-chat-latest})
and ATP systems (default Vampire~\cite{BB+25} for both theorem proving and model finding), setting
the acceptance, convergence, and zigzagging thresholds (defaults 0.74, 0.94, 0.94), setting the
maximal number of failures in a Zig (default 10), number of zigzags in a sequence (default 10), 
and number of zigzagging repetitions (default 3).
Initial testing has been done on test data provided

%--------------------------------------------------------------------------------------------------
\bibliographystyle{flairs}
\bibliography{Bibliography.bib}
%--------------------------------------------------------------------------------------------------
\end{document}
